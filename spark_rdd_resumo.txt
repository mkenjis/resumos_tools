Source for more details:
https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html


Resilient Distributed Datasets (RDDs)

RDD is the core data abstraction in Spark. It represents a collection of objects that is
- Immutable (read-only)
- Resilient (fault-tolerant)
- Distributed (dataset spread out to more than one node)

There are two types of RDD operations: 
* Transformations - produce a new RDD by performing some data manipulation on source RDD. 
* Actions - trigger a computation to return the result to the calling program or to perform some actions on an RDD’s elements.


SparkContext
============

import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

val conf = new SparkConf().setAppName("Counting Lines").setMaster("spark://master:7077")
val sc = new SparkContext(conf)
 

Create RDDs
===========
val numberRDD = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)  // from local collection
val licLines = sc.textFile("numbers.txt")                      // reading from a file


Manipulate RDDs
===============

1) map(func: (T) => U): RDD[U] - returns a RDD with elements transformed by func 

numberRDD.map(num => num * num)


2) flatMap(func: (T) => Iterable[U]): RDD[U] - func must return a Iterable
                                             - returns a RDD with elements flatten out
val idsStr = licLines.flatMap(_.split(","))
idsStr.collect()
res11: Array[String] = Array(15, 16, 20, 20, 77, 80, 94, 94, 98, 16, 31, 31, 15, 20)


3) filter(func: (T)): RDD[T] - returns a RDD with elements that satisfy func

licLines.filter(line => line.contains("BSD"))


4) reduce(func: (V, V) => V): V - returns a single value
                                - func must be commutative and associative binary operator.
numberRDD.reduce( (x, y) => x + y )   // returns 55


5) distinct(): RDD[T] - returns a RDD with unique elements

val intIds = idsStr.map(_.toInt)
intIds.distinct.collect()
res15: Array[Int] = Array(16, 80, 98, 20, 94, 15, 77, 31


6) take(N), 
   top(N, [order]),
   takeOrdered(N, [order]) - returns N values from RDD to driver program


7) collect(), 
   first(), 
   last(), 
   count() - returns values to driver program



Create Pair RDDs
================

Pair RDD is created as RDD 2-element tuple, ie, RDD[(K,V)]
RDD 2-element tuple get implicitly converted to an instance of class PairRDDFunctions

val pairdd = rdd.map( x => (K, V) )
val pairdd = rdd.keyBy(f(x))

val tranFile = sc.textFile("first-edition/ch04/ch04_data_transactions.txt") 
val tranData = tranFile.map(_.split("#")) 
var transByCust = tranData.map(tran => (tran(2).toInt, tran))

transByCust.collect().foreach(tran => println(tran.mkString(", ")))
2015-03-30, 6:18 AM, 53, 42, 5, 2197.85
2015-03-30, 4:42 AM, 53, 3, 6, 9182.08

Manipulate Pair RDDs
====================

1) keys   - returns scala.collection.Array[K]
   values - returns scala.collection.Array[V]

transByCust.keys.distinct().count()
res0: Long = 100


2) countByKey(): Dict[K, int] - returns scala.collection.Map[Int,Long]

transByCust.countByKey()
res1: scala.collection.Map[Int,Long] = Map(69 -> 7, 88 -> 5, 5 -> 11, 
10 -> 7, 56 -> 17, 42 -> 7, 24 -> 9, 37 -> 7, 25 -> 12, 52 -> 9, 14 -> 8, 
20 -> 8, 46 -> 9, 93 -> 12, 57 -> 8, 78 -> 11, 29 -> 9, 84 -> 9, 61 -> 8, 
89 -> 9, 1 -> 9, 74 -> 11, 6 -> 7, 60 -> 4,...


3) lookup(K): RDD[(K, U)] - returns all elements with key = <key>

transByCust.lookup(53)
res1: Seq[Array[String]] = WrappedArray(Array(2015-03-30, 6:18 AM, 53, 42, 5, 2197.85), Array(2015-03-30, 4:42 AM, 53, 3, 6, 9182.08), ...


4) mapValues(func: (V) => U): RDD[(K, U)] - changes only value part in a pair RDD and keeping associated keys untouched

transByCust = transByCust.mapValues(tran => {
	 if(tran(3).toInt == 25 && tran(4).toDouble > 1) 
		tran(5) = (tran(5).toDouble * 0.95).toString
	 tran 
	 }


5) flatMapValues(func: (V) => Iterable[U]): RDD[(K, U)] - changes number of elements for a key by mapping each key to 0+ values.

transByCust = transByCust.flatMapValues(tran => { 
	 if(tran(3).toInt == 81 && tran(4).toDouble >= 5) { 
		val cloned = tran.clone()
		cloned(5) = "0.00"; cloned(3) = "70"; cloned(4) = "1"; 
		List(tran, cloned)
	 } 
	 else 
		List(tran)
	 }

6) reduceByKey(func: (V, V) => V): RDD[(K, V)] - merges all values for a key into a single value


7) foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V)] - same as reduceByKey, but it requires an additional parameter, zeroValue, in the parameter list 

val amounts = transByCust.mapValues(t => t(5).toDouble)
val totals = amounts.foldByKey(0)((p1, p2) => p1 + p2).collect()
res0: Array[(String, Double)] = Array((84,53020.619999999995), (96,36928.57), (66,52130.01), (54,36307.04), ...


8) aggregateByKey(zeroValue: V)(seqFunc: (U, V) => U, mergeFunc: (U, U) => U): RDD[(K, U)] - similar to foldByKey and reduceByKey.

- merges values and transforms values to another type. 

- takes zeroValue argument and two functions arguments: 
  * first transforms values from type V to type U  (with the signature U, V) => U) 
  * seconde merges transformed values (with the signature (U, U) => U).

val prods = transByCust.aggregateByKey(List[String]())( 
		 (prods, tran) => prods ::: List(tran(3)),
		 (prods1, prods2) => prods1 ::: prods2) 
prods.collect()
res0: Array[(String, List[String])] = Array((88,List(47.149.147.123, 74.211.5.196,...), (82,List(8.140.151.84, 23.130.185.187,...), ...)


RDD Partitioners  - partitions RDDs by assigning a partition index to each RDD element 
================  - spark.default.parallelism is used if number of partitions not specified

* HashPartitioner ( partitionIndex = hashCode % numberOfPartitions )
* RangePartitioner ( partitions data of sorted RDDs into roughly equal ranges )
* Custom partitioners only on Pair RDDs

pairrdd.foldByKey(afunction, 100)
pairrdd.foldByKey(afunction, new HashPartitioner(100))

pairrdd.aggregateByKey(zeroValue, 100)(seqFunc, comboFunc).collect()
pairrdd.aggregateByKey(zeroValue, new CustomPartitioner())(seqFunc, comboFunc).collect()


Repartitioning RDDs
===================
* partitionBy (Partitioner: partitioner) - use Partitioner object to partition pair RDDs 
* coalesce (numPartitions: Int, shuffle: Boolean = false) - merges partitions and does not shuffle
* repartition (numPartitions: Int) - repartition and shuffle partitions
* repartitionAndSortWithinPartition()


Mapping RDD Partitions
======================
* mapPartitions ( f(Iterator[T]) => Iterator[U]): RDD[(U)] - function applies transformation on elements of Iterator[T} of each RDD partitions
* mapPartitionsWithIndex ( f(Int, Iterator[T])) => Iterator[U]): RDD[(U)]
* glom(): RDD[ (List[T]) ] - gathers elements of each partition into an array and returns a new RDD with those arrays as elements.

val list = List.fill(500)(scala.util.Random.nextInt(100))
list: List[Int] = List(88, 59, 78, 94, 34, 47, 49, 31, 84, 47, ...)
val rdd = sc.parallelize(list, 30).glom()
rdd: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[0]
rdd.collect()
res0: Array[Array[Int]] = Array(Array(88, 59, 78, 94,...), ...)


Joining RDDs
============

1) Consider RDD(K, V) joined with a RDD(K, W) :

* join — returns elements of type (K, (V, W)) matching the keys in the first and second RDDs. For non-matching keys, no elements will be returned.

* leftOuterJoin — returns elements of type (K, (V,Option(W))). For non-matching keys in the second RDD, it returns elements  (key, (v, None)).

* rightOuterJoin — returns elements of type (K, (Option(V), W)). For non-matching keys in the first RDD, it returns elements (key, (None, w)).

* fullOuterJoin — returns elements of type (K, (Option(V), Option(W)). For non-matching keys in the first or second RDD, it returns both elements (key, (v, None)) and (key, (None, w)).

val totalsAndProds = totalsByProd.join(products)
totalsAndProds.first()
res0: (Int, (Double, Array[String])) = (84,(75192.53,Array(84, Cyanocobalamin, 2044.61, 8)))

val totalsWithMissingProds = products.leftOuterJoin(totalsByProd)
val totalsWithMissingProds = totalsByProd.rightOuterJoin(products)


2) subtract And subtractByKey

* subtract - returns elements from the first RDD that aren’t present in the second one. It compares complete elements (not just their keys or values)
* subtractByKey - works on pair RDDs and returns elements from the first RDD whose keys aren’t in the second RDD. The second RDD doesn’t need to have values of the same type as the first one.

val missingProds = products.subtractByKey(totalsByProd).values


3) cogroup Transformation

- performs a grouping of values by key from each RDDs. 
- returns an pair RDD whose values are arrays of Iterable objects containing values from each RDD by key.

cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)]): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]

val prodTotCogroup = totalsByProd.cogroup(products)
prodTotCogroup: org.apache.spark.rdd.RDD[(Int, (Iterable[Double], Iterable[Array[String]]))]...


4) zip Transformation

- zip creates an RDD of pairs (T, U) with the first pair having the first elements from each RDD, the second pair having the second elements, and soon.
- zip requires RDDs have same number of partitions and same number of elements in them.

val rdd1 = sc.parallelize(List(1,2,3))
val rdd2 = sc.parallelize(List("n4","n5","n6"))
rdd1.zip(rdd2).collect()
res1: Array[(Int, Int)] = Array((1,"n4"), (2,"n5"), (3,"n6")


5) zipPartition Transformation

- zipPartitions enables you to zip over elements inside partitions
- all RDDs need to have the same number of partitions (but not the same number of elements in them).
- zipPartitions accepts two sets of arguments :
  * in the first, you give it RDDs; 
  * in the second, a function that takes a matching number of Iterator objects used for accessing elements in each partition. 
- function must return a new Iterator, which can be a different type (matching the resulting RDD).

val rdd1 = sc.parallelize(1 to 10, 10)
val rdd2 = sc.parallelize((1 to 8).map(x=>"n"+x), 10)
rdd1.zipPartitions(rdd2, true)((iter1, iter2) => {
		 iter1.zipAll(iter2, -1, "empty").map({case(x1, x2)=>x1+"-"+x2}) 
		 }).collect()
res1: Array[String] = Array(1-empty, 2-n1, 3-n2, 4-n3, 5-n4, 6-empty, 7-n5, 8-n6, 9-n7, 10-n8


Sorting data
============

* sortByKey
* sortBy
* repartitionAndSortWithinPartition

val sortedProds = totalsAndProds.sortBy(_._2._2(1)).collect()
res0: Array[(Double, Array[String])] = Array((90,(48601.89,Array(90, AMBROSIA TRIFIDA POLLEN, 5887.49, 1))), 
(94,(31049.07,Array(94, ATOPALM MUSCLE AND JOINT, 1544.25, 7))), (87,(26047.72,Array(87, Acyclovir, 6252.58, 4))), ..


1) Sorting with Ordered Trait

- class extending Ordered has to implement the compare function, taking as an argument object of the same class to compare against.
 
case class Employee(lastName: String) extends Ordered[Employee] {
  override def compare(that: Employee) = this.lastName.compare(that.lastName)
}


2) Sorting with Ordering Trait

- define an object of type Ordering[T] somewhere in the scope of the function calling sortByKey. For example :

implicit val emplOrdering = new Ordering[Employee] {
  override def compare(a: Employee, b: Employee) = a.lastName.compare(b.lastName)
}

or

implicit val emplOrdering: Ordering[Employee] = Ordering.by(_.lastName) 

If defined within its scope, this implicit object will be picked up by the sortByKey transformation (called on an RDD with keys of type Employee), and the RDD will become sortable.


RDD Dependencies
================
- Spark execution follows DAG model. Each RDD transformation produces a new RDD. 
- New RDD depends on the old one. This graph of dependencies is called RDD lineage.
- Spark packages work to be sent to executors. Every job is divided into stages based on the points where shuffles occur.
- For each stage and each partition, tasks are created and sent to the executors to be processed. 


Checkpoint RDD
==============
- Checkpoint is a way Spark provides to persist the entire RDD to stable storage.
- In case node failure, Spark uses the snapshot saved and doesn’t need to recompute the missing RDD pieces from the start.

* rdd.checkpoint() - entire RDD is persisted to disk and RDD´s dependencies are erased.
* sc.setCheckpointDir(<path>)
