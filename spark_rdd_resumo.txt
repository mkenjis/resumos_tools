Source for more details:
https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html


Resilient Distributed Datasets (RDDs)

RDD is the core data abstraction in Spark. It represents a collection of objects that is
- Immutable (read-only)
- Resilient (fault-tolerant)
- Distributed (dataset spread out to more than one node)

There are two types of RDD operations: 
* Transformations - produce a new RDD by performing some data manipulation on source RDD. 
* Actions - trigger a computation to return the result to the calling program or to perform some actions on an RDD’s elements.


Manipulating Spark RDDs
=======================

1) Creating SparkContext instance

import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

val conf = new SparkConf().setAppName("Counting Lines").setMaster("spark://master:7077")
val sc = new SparkContext(conf)
 

2) Creating RDDs

val licLines = sc.textFile("numbers.txt")
val numberRDD = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)


3) rdd.map(func: (T) => U): RDD[U]

Returns a RDD with each element transformed by func.

val numbersSquared = numberRDD.map(num => num * num)


4) rdd.flatMap(func: (T) => Iterable[U]): RDD[U]

Returns a RDD with elements of Iterable flatten out.

scala> val ids = licLines.flatMap(_.split(","))
scala> ids.collect
res11: Array[String] = Array(15, 16, 20, 20, 77, 80, 94, 94, 98, 16, 31, 31, 15, 20)


5) rdd.filter(func: (T)): RDD[T]

Returns a RDD with elements that satisfy func condition.

scala> val bsdLines = licLines.filter(line => line.contains("BSD"))


6) rdd.reduce(func: (V, V) => V): V

Reduces the RDD elements using the specified commutative and associative binary operator.

scala> val numberRDD = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)
scala> numberRDD.reduce( (x, y) => x + y )


7) rdd.distinct(): RDD[T]

Returns a RDD with unique elements

scala> val intIds = ids.map(_.toInt)
scala> val uniqueIds = intIds.distinct
scala> uniqueIds.collect
res15: Array[Int] = Array(16, 80, 98, 20, 94, 15, 77, 31


8) rdd.take(N), rdd.takeOrdered(N, [order]), rdd.top(N, [order])

Returns N records from RDD to driver program.


9) rdd.collect(), rdd.first(), rdd.last(), rdd.count()



Manipulating Pair RDDs
======================

1) Creating Pair RDDs

RDDs created as two-element tuples (or pair RDDs) get implicitly converted to an instance of class PairRDDFunctions.
Pair RDD functions automatically become available through the implicit conversion.

scala> val pairdd = rdd.map( x => (K, V) )
scala> val pairdd = rdd.keyBy(f(x))

scala> val tranFile = sc.textFile("first-edition/ch04/ch04_data_transactions.txt") 
scala> val tranData = tranFile.map(_.split("#")) 
scala> var transByCust = tranData.map(tran => (tran(2).toInt, tran))


2) Getting Keys And Values

pairdd.keys - returns scala.collection.Array[K]
pairdd.values - returns scala.collection.Array[V]

scala> transByCust.keys.distinct().count()
res0: Long = 100


3) pairrdd.countByKey(): Dict[K, int]

Returns scala.collection.Map[Int,Long]

scala> transByCust.countByKey()
res1: scala.collection.Map[Int,Long] = Map(69 -> 7, 88 -> 5, 5 -> 11, 
10 -> 7, 56 -> 17, 42 -> 7, 24 -> 9, 37 -> 7, 25 -> 12, 52 -> 9, 14 -> 8, 
20 -> 8, 46 -> 9, 93 -> 12, 57 -> 8, 78 -> 11, 29 -> 9, 84 -> 9, 61 -> 8, 
89 -> 9, 1 -> 9, 74 -> 11, 6 -> 7, 60 -> 4,...


4) pairrdd.lookup(K): RDD[(K, U)]

Returns all elements with key = <key>

scala> transByCust.lookup(53)
res1: Seq[Array[String]] = WrappedArray(Array(2015-03-30, 6:18 AM, 53, 42, 5, 2197.85), Array(2015-03-30, 4:42 AM, 53, 3, 6, 9182.08), ...


5) pairrdd.mapValues(func: (V) => U): RDD[(K, U)]

Changes the values contained in a pair RDD without changing the associated keys

scala> transByCust = transByCust.mapValues(tran => {
		 if(tran(3).toInt == 25 && tran(4).toDouble > 1) 
			tran(5) = (tran(5).toDouble * 0.95).toString
		 tran 
		 }


6) pairrdd.flatMapValues(func: (V) => Iterable[U]): RDD[(K, U)]

Enables to change the number of elements corresponding to a key by mapping each key to 0+ values.

scala> transByCust = transByCust.flatMapValues(tran => { 
		 if(tran(3).toInt == 81 && tran(4).toDouble >= 5) { 
			val cloned = tran.clone()
			cloned(5) = "0.00"; cloned(3) = "70"; cloned(4) = "1"; 
			List(tran, cloned)
		 } 
		 else 
			List(tran)
		 }

7) pairrdd.reduceByKey(func: (V, V) => V): RDD[(K, V)]

Merges all the values of a key into a single value of the same type.


8) pairrdd.foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V)]

Same as reduceByKey, except that it requires an additional parameter, zeroValue, that comes before the reduce function. 

scala> val amounts = transByCust.mapValues(t => t(5).toDouble)
scala> val totals = amounts.foldByKey(0)((p1, p2) => p1 + p2).collect()
res0: Array[(String, Double)] = Array((84,53020.619999999995), (96,36928.57), (66,52130.01), (54,36307.04), ...


9) pairrdd.aggregateByKey(zeroValue: V)(seqFunc: (U, V) => U, mergeFunc: (U, U) => U): RDD[(K, U)]

Similar to foldByKey and reduceByKey.

Merges values and takes zeroValue argument, but it also transforms values to another type. 

Also it takes two functions as arguments: a transform function for transforming values from type V to type U (with the signature (U, V) => U) and a merge function for merging the transformed values (with the signature (U, U) => U).

scala> val prods = transByCust.aggregateByKey(List[String]())( 
			 (prods, tran) => prods ::: List(tran(3)),
			 (prods1, prods2) => prods1 ::: prods2) 
scala> prods.collect()
res0: Array[(String, List[String])] = Array((88,List(47.149.147.123, 74.211.5.196,...), (82,List(8.140.151.84, 23.130.185.187,...), ...)


Spark’s data partitioners
=========================
Partitioner objects available that assign a partition index to each RDD element :

* HashPartitioner ( partitionIndex = hashCode % numberOfPartitions )
* RangePartitioner ( partitions data of sorted RDDs into roughly equal ranges )
* Pair RDDs also accept custom partitioners
* spark.default.parallelism used if number of partitions not specified

pairrdd.foldByKey(afunction, 100)
pairrdd.foldByKey(afunction, new HashPartitioner(100))

pairrdd.aggregateByKey(zeroValue, 100)(seqFunc, comboFunc).collect()
pairrdd.aggregateByKey(zeroValue, new CustomPartitioner())(seqFunc, comboFunc).collect()


Repartitioning RDDs
===================
* partitionBy (Partitioner: partitioner ) - available only on pair RDDs
* coalesce (numPartitions: Int, shuffle: Boolean = false)
* repartition (numPartitions: Int)
* repartitionAndSortWithinPartition


Mapping data in partitions
==========================
* mapPartitions (f(Iterator[T]) => Iterator[U]): RDD[(U)] - apply a function to each of its partitions separately.
* mapPartitionsWithIndex (f(Int, Iterator[T])) => Iterator[U]): RDD[(U)]
* glom(): RDD[ (List[T]) ] - gathers elements of each partition into an array and returns a new RDD with those arrays as elements.

scala> val list = List.fill(500)(scala.util.Random.nextInt(100))
list: List[Int] = List(88, 59, 78, 94, 34, 47, 49, 31, 84, 47, ...)
scala> val rdd = sc.parallelize(list, 30).glom()
rdd: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[0]
scala> rdd.collect()
res0: Array[Array[Int]] = Array(Array(88, 59, 78, 94,...), ...)


Joining RDD data
================

1) Consider RDD (K, V) elements, and passing in an RDD (K, W) elements for a join :

* join — this returns a new pair RDD with the elements (K, (V, W)) containing all possible pairs of values from the first
and second RDDs that have the same keys. For the keys that exist in only one of the two RDDs, the resulting RDD will have no elements.

* leftOuterJoin — this returns elements of type (K, (V,Option(W))); The resulting RDD will also contain the elements (key, (v, None)) for those keys that don’t exist in the second RDD.

* rightOuterJoin — this returns elements of type (K, (Option(V), W)); the resulting RDD will also contain the elements (key, (None, w)) for those keys that don’t exist in the first RDD.

* fullOuterJoin — this returns elements of type (K, (Option(V), Option(W)); the resulting RDD will contain both (key, (v, None)) and (key, (None, w)) elements for those keys that exist in only one of the two RDDs.

scala> val totalsAndProds = totalsByProd.join(products)
scala> totalsAndProds.first()
res0: (Int, (Double, Array[String])) = (84,(75192.53,Array(84, Cyanocobalamin, 2044.61, 8)))

val totalsWithMissingProds = products.leftOuterJoin(totalsByProd)
val totalsWithMissingProds = totalsByProd.rightOuterJoin(products)


2) subtract And subtractByKey

* subtract - returns elements from the first RDD that aren’t present in the second one. It works on ordinary RDDs and compares complete elements (not just their keys or values)

* subtractByKey - works on pair RDDs and returns an RDD with pairs from the first RDD whose keys aren’t in the second RDD. The second RDD doesn’t need to have values of the same type as the first one.

scala> val missingProds = products.subtractByKey(totalsByProd).values


3) cogroup Transformation

Performs a grouping of values from each RDDs by key. Returns an RDD whose values are arrays of Iterable objects containing values from each RDD. So, cogroup groups values of several RDDs by key and then joins these grouped RDDs.

For example, the signature of the cogroup function for cogrouping three RDDs (including the enclosing one) is as follows:

cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)]):
 RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]

scala> val prodTotCogroup = totalsByProd.cogroup(products)
prodTotCogroup: org.apache.spark.rdd.RDD[(Int, (Iterable[Double], Iterable[Array[String]]))]...


4) zip Transformation

zip functions just like the zip function in Scala: if you call rdd1[T].zip(rdd2[U]), it will create an RDD of pairs (T, U) with the first pair having the first elements from each RDD, the second pair having the second elements, and soon.

scala> val rdd1 = sc.parallelize(List(1,2,3))
scala> val rdd2 = sc.parallelize(List("n4","n5","n6"))
scala> rdd1.zip(rdd2).collect()
res1: Array[(Int, Int)] = Array((1,"n4"), (2,"n5"), (3,"n6")


5) zipPartition Transformation

zipPartitions enables you to iterate over elements inside partitions, but you use it to combine several RDDs’ partitions. All RDDs need to have the same number of partitions (but not the same number of elements in them).

zipPartitions accepts two sets of arguments. In the first set, you give it RDDs; and in the second, a function that takes a matching number of Iterator objects used for accessing elements in each partition. The function must return a new Iterator, which can be a different type (matching the resulting RDD).

scala> val rdd1 = sc.parallelize(1 to 10, 10)
scala> val rdd2 = sc.parallelize((1 to 8).map(x=>"n"+x), 10)
scala> rdd1.zipPartitions(rdd2, true)((iter1, iter2) => {
		 iter1.zipAll(iter2, -1, "empty").map({case(x1, x2)=>x1+"-"+x2}) 
		 }).collect()
res1: Array[String] = Array(1-empty, 2-n1, 3-n2, 4-n3, 5-n4, 6-empty, 7-n5, 8-n6, 9-n7, 10-n8


Sorting data
============

* sortByKey
* sortBy
* repartitionAndSortWithinPartition

scala> val sortedProds = totalsAndProds.sortBy(_._2._2(1))
scala> sortedProds.collect()
res0: Array[(Double, Array[String])] = Array((90,(48601.89,Array(90, 
AMBROSIA TRIFIDA POLLEN, 5887.49, 1))), (94,(31049.07,Array(94, ATOPALM 
MUSCLE AND JOINT, 1544.25, 7))), (87,(26047.72,Array(87, Acyclovir, 
6252.58, 4))), ..


1) Making A Class Orderable Using The Ordered Trait

The class extending Ordered has to implement the compare function, which takes as an argument an object of the same class against which to perform the comparison.

The sortByKey transformation requires an argument of type Ordering, but there’s an implicit conversion in Scala from Ordered to Ordering so you can safely use this method.
 
case class Employee(lastName: String) extends Ordered[Employee] {
 override def compare(that: Employee) = this.lastName.compare(that.lastName)
}


2) Making A Class Orderable Using The Ordering Trait

Define an object of type Ordering[Employee] somewhere in the scope of the function calling sortByKey. For example :

implicit val emplOrdering = new Ordering[Employee] {
 override def compare(a: Employee, b: Employee) = a.lastName.compare(b.lastName)
}

or

implicit val emplOrdering: Ordering[Employee] = Ordering.by(_.lastName) 

If defined within its scope, this implicit object will be picked up by the sortByKey transformation (called on an RDD with keys of type Employee), and the RDD will become sortable.


Checkpoint RDD
==============
* rdd.checkpoint() - entire RDD is persisted to disk and RDD´s dependencies are erased.
* sc.setCheckpointDir(<path>)
