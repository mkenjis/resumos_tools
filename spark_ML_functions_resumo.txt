Source for more details:
https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html


ML with Spark MLlib API
=======================

MLlib API was introduce in Spark 0.8.

Basically it works on RDD API to provide the ML functionalities.


1) Preparing the data

import org.apache.spark.mllib.linalg.Vectors
val housingLines = sc.textFile("first-edition/ch07/housing.data", 6)
val housingVals = housingLines.map(x => Vectors.dense(x.split(",").map(_.trim().toDouble)))


2) Analyzing data distribution

- column statistics

import org.apache.spark.mllib.linalg.distributed.RowMatrix
val housingMat = new RowMatrix(housingVals)
val housingStats = housingMat.computeColumnSummaryStatistics()  # returns a MultivariateStatisticalSummary object

housingStats.min / max / mean / variance / normL1 / normL2

- column cosine similarities

val housingColSims = housingMat.columnSimilarities()

- covariance matrix

val housingCovar = housingMat.computeCovariance()


3) Transforming to labeled points

Dataset needs to be converted to LabeledPoint class (used in most of Spark’s machine-learning algorithms). 
It contains target value and feature Vector.

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
val housingData = housingVals.map(x => { 
   val a = x.toArray
   LabeledPoint(a(a.length-1), Vectors.dense(a.slice(0, a.length-1))) 
})


4) Splitting the data

val sets = housingData.randomSplit(Array(0.8, 0.2))
val housingTrain = sets(0)
val housingValid = sets(1)


5) Feature scaling / mean normalization

Feature scaling means the ranges of data are scaled to comparable sizes.
Mean normalization means the data is translated so that the averages are roughly zero.

import org.apache.spark.mllib.feature.StandardScaler 
val scaler = new StandardScaler(true, true).fit(housingTrain.map(x => x.features)

val trainScaled = housingTrain.map(x => LabeledPoint(x.label, scaler.transform(x.features)))
val validScaled = housingValid.map(x => LabeledPoint(x.label, scaler.transform(x.features))


6) Fitting a linear regression model

import org.apache.spark.mllib.regression.LinearRegressionWithSGD

val model = LinearRegressionWithSGD.train(trainScaled, 200, 1.0)

or

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true) 
alg.optimizer.setNumIterations(200)
trainScaled.cache() 
validScaled.cache() 
val model = alg.run(trainScaled


7) Predicting the target values

val validPredicts = validScaled.map(x => (model.predict(x.features), x.label))


8) Evaluating the model’s performance

scala> import org.apache.spark.mllib.evaluation.RegressionMetrics 
scala> val validMetrics = new RegressionMetrics(validPredicts)
scala> validMetrics.rootMeanSquaredError
res1: Double = 4.775608317676729

scala> validMetrics.meanSquaredError
res2: Double = 22.806434803863162


9) Loading and saving the model

- Saving

model.save(sc, "chapter07output/model")

- Loading

import org.apache.spark.mllib.regression.LinearRegressionModel
val model = LinearRegressionModel.load(sc, "ch07output/model")



ML with Spark ML API
====================

ML API was introduce in Spark 1.2. Spark ML uses DataFrame to work with datasets.

MLlib API wasn’t scalable and extendable enough, nor sufficiently practical for use in real machine learning projects. The goal of Spark ML library is to generalize machine learning operations and streamline machine learning processes.

ML API introduces new abstractions — estimators, transformers, and evaluators — that can be combined to form pipelines.


1) Preparing the data

val df_raw = spark.read.format("csv").
             option("header","true").option("inferSchema","true").
             load("first-edition/ch08/adult.raw")

 
2) Dealing With Categorical Values

* StringIndexer() - converts String categorical values into integer indexes of those values. Takes a DataFrame and fits a StringIndexerModel, which is then used for transformations of a column.

import org.apache.spark.ml.feature.StringIndexer
val si = new StringIndexer().setInputCol("col").setOutputCol("newcol")
val sm = si.fit(df)
val newdf = sm.transform(df)

* OneHotEncoder() - one-hot encodes a column and puts the results into a new column as a one-hot-encoded sparse Vector.

import org.apache.spark.ml.feature.OneHotEncoder
val onehotenc = new OneHotEncoder().setInputCol("col").setOutputCol("newcol")
val newdf = onehotenc.transform(df)


3) Merging The Data

* VectorAssembler() - merge all Vectors and columns into a single Vector column containing all the features.

import org.apache.spark.ml.feature.VectorAssembler
val va = new VectorAssembler().setInputCols(df.columns.diff(Array("income"))).setOutputCol("features")
val points = va.transform(df)


4) Splitting the data

val splits = points.randomSplit(Array(0.8, 0.2))
val adulttrain = splits(0).cache()
val adultvalid = splits(1).cache(


5) Training the logistic regression model

import org.apache.spark.ml.LogisticRegression
val lr = new LogisticRegression.setRegParam(0.01).setMaxIter(500).setFitIntercept(true)

val lrmodel = lr.fit(adulttrain)


6) Predicting the target values

val validpredicts = lrmodel.transform(adultvalid)
validpredicts.show()


7) Evaluating model´s performance

scala> import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
scala> val bceval = new BinaryClassificationEvaluator()
bceval: org.apache.spark.ml...

scala> bceval.evaluate(validpredicts)
res0: Double = 0.9039934862200736

scala> bceval.getMetricName
res1: String = areaUnderROC


8) Precision And Recall Metrics

P =   TP    (precision)
    -------
	TP + FP
	
R =   TP    (recall)
    -------
	TP + FN
	
f1 =   2PR
     -------
	 P + RDD
	 

9) PR Curve And ROC Curve

A precision-recall (PR) curve is calculated changing the probability threshold gradually and counting 0 and 1 labels according to probability found at each point, and calculate precision and recall. Then you plot the obtained values on the same graph (precision on the Y-axis and recall on the X-axis)

The ROC curve is similar to the PR curve, but it has recall (TPR) plotted on its Y-axis and the false positive rate (FPR) plotted on its X-axis. FPR is calculated as the percentage of false positives out of all negative samples

FPR =   FP
      -------
	  FP + TN


10) Loading and saving the model

- Persist model and pipeline

lrmodel.write.overwrite().save("/tmp/logistic-regression-model")


- load model and pipeline

val prevModel = LogisticRegressionModel.load("/tmp/spark-logistic-regression-model")


	  
11) K-Fold Cross-Validation

import org.apache.spark.ml.tuning.CrossValidator
val cv = new CrossValidator().setEstimator(lr).
    setEvaluator(bceval).setNumFolds(5

import org.apache.spark.ml.tuning.ParamGridBuilder
val paramGrid = new ParamGridBuilder().
    addGrid(lr.maxIter, Array(1000)).
    addGrid(lr.regParam, Array(0.0001, 0.001, 0.005, 0.01, 0.05, 0.1)).
    build()

cv.setEstimatorParamMaps(paramGrid)

val cvmodel = cv.fit(adulttrain

* Getting Best Model And Evaluating

import org.apache.spark.ml.classification.LogisticRegressionModel
val bestmodel = cvmodel.bestModel.asInstanceOf[LogisticRegressionModel]

bestmodel.coefficients
best.get* (RegParam, Threshold, Iterations, etc)

val eval = new BinaryClassificationEvaluator()
eval.evaluate(bestmodel.transform(adultvalid))
