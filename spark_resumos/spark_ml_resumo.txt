Spark ML API
============

ML API (introduced in Spark 1.2) uses DataFrame to work with datasets.

ML API introduces new abstractions — estimators, transformers, and evaluators — that can be combined to form pipelines.


- load data

val df_raw = spark.read.format("csv").
             option("header","true").option("inferSchema","true").
             load("first-edition/ch08/adult.raw")

 
- Categorical Values

* StringIndexer() - converts String categorical values into integer indexes by fitting a StringIndexerModel for the column

import org.apache.spark.ml.feature.StringIndexer
val si = new StringIndexer().setInputCol("col").setOutputCol("newcol")
val sm = si.fit(df)
val newdf = sm.transform(df)

* OneHotEncoder() - one-hot encodes a column and puts the results into a new column as sparse Vector.

import org.apache.spark.ml.feature.OneHotEncoder
val onehotenc = new OneHotEncoder().setInputCol("col").setOutputCol("newcol")
val newdf = onehotenc.transform(df)


- VectorAssembler() - merge all Vectors and columns into a single Vector column containing all the features.

import org.apache.spark.ml.feature.VectorAssembler
val va = new VectorAssembler().setInputCols(df.columns.diff(Array("income"))).setOutputCol("features")
val points = va.transform(df)


- split dataset

val splits = points.randomSplit(Array(0.8, 0.2))
val adulttrain = splits(0).cache()
val adultvalid = splits(1).cache()


- fit a logistic regression model

import org.apache.spark.ml.LogisticRegression
val lr = new LogisticRegression.setRegParam(0.01).setMaxIter(500).setFitIntercept(true)

val lrmodel = lr.fit(adulttrain)


- predict & evaluating model performance

val validpredicts = lrmodel.transform(adultvalid)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(validpredicts)
res0: Double = 0.9039934862200736

bceval.getMetricName
res1: String = areaUnderROC


- Precision And Recall Metrics

P =   TP    (precision)
    -------
	TP + FP
	
R =   TP    (recall)
    -------
	TP + FN
	
f1 =   2PR
     -------
	 P + RDD
	 

- PR Curve And ROC Curve

A precision-recall (PR) curve is calculated changing the probability threshold gradually and counting 0 and 1 labels according to probability found at each point, and calculate precision and recall. Then you plot the obtained values on the same graph (precision on the Y-axis and recall on the X-axis)

The ROC curve is similar to the PR curve, but it has recall (TPR) plotted on its Y-axis and the false positive rate (FPR) plotted on its X-axis. FPR is calculated as the percentage of false positives out of all negative samples

FPR =   FP
      -------
	  FP + TN


- save / load the model

* save model / pipeline

lrmodel.write.overwrite().save("/tmp/logistic-regression-model")


* load model / pipeline

val prevModel = LogisticRegressionModel.load("/tmp/spark-logistic-regression-model")


	  
- K-Fold Cross-Validation

import org.apache.spark.ml.tuning.CrossValidator
val cv = new CrossValidator().setEstimator(lr).
    setEvaluator(bceval).setNumFolds(5

import org.apache.spark.ml.tuning.ParamGridBuilder
val paramGrid = new ParamGridBuilder().
    addGrid(lr.maxIter, Array(1000)).
    addGrid(lr.regParam, Array(0.0001, 0.001, 0.005, 0.01, 0.05, 0.1)).
    build()

cv.setEstimatorParamMaps(paramGrid)

val cvmodel = cv.fit(adulttrain

* Getting Best Model And Evaluating

import org.apache.spark.ml.classification.LogisticRegressionModel
val bestmodel = cvmodel.bestModel.asInstanceOf[LogisticRegressionModel]

bestmodel.coefficients
best.get* (RegParam, Threshold, Iterations, etc)

val eval = new BinaryClassificationEvaluator()
eval.evaluate(bestmodel.transform(adultvalid))


- other Transformers and Estimators

* TRANSFORMERS

- Binarizer() - transforms input column values into two groups (0/1).

import org.apache.spark.ml.feature.Binarizer
val binarizer = new Binarizer().setThreshold(35.6)
                .setInputCol("temperature").setOutputCol("freezing")
binarizer.transform(arrival_data).show

- Bucketizer() - transforms input column values into buckets specified by an array of double values.

import org.apache.spark.ml.feature.Bucketizer
val bucketer = new Bucketizer().setSplits(bucketBorders)
               .setInputCol("temperature").setOutputCol("intensity")
val output = bucketer.transform(arrival_data)

- Tokenizer() - performs tokenization on a string separated by spaces. If a different delimiter is used, use RegexTokenizer.

import org.apache.spark.ml.feature.Tokenizer
val tokenizer = new Tokenizer().setInputCol("line").setOutputCol("words")
val tokenized = tokenizer.transform(text_data)

- StopWordsRemover() - removes stop words from a built-in or user-provided set after the tokenization.

import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords)
              .setInputCol("words").setOutputCol("filtered")
val cleanedTokens = remover.transform(tokenized)

- HashingTF() - transforms words into a numeric representation (by applying a hash function) and computing the frequency of each word in each line.

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setNumFeatures(4096)
             .setInputCol("filtered").setOutputCol("TFOut")
val tfResult = tf.transform(cleanedTokens)

* ESTIMATORS

- RFormula() - converts both numeric and categorial values to label and feature vector by expressing the transformation logic declaretively. 

import org.apache.spark.ml.feature.RFormula
val formula = new RFormula().setFormula("arrival ~ . + hour:temperature")
              .setFeaturesCol("features").setLabelCol("label")
val output = formula.fit(arrival_data).transform(arrival_data)

- IDF() - computes the importance or weight of each word by counting the number of documents it appears in.

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("wordFreqVect")
          .setOutputCol("features")
val idfModel = idf.fit(tfResult)
val weightedWords = idfModel.transform(tfResult)

- MinMaxScaler() - rescales each column individually to a common range of values of min and max using the column summary statistics.

import org.apache.spark.ml.feature.MinMaxScaler
val minMaxScaler = new MinMaxScaler().setMin(0.0).setMax(5.0)
                   .setInputCol("features").setOutputCol("scaledFeatures")
val scalerModel = minMaxScaler.fit(employee_data)
val scaledData = scalerModel.transform(employee_data)