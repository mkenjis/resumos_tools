Source for more details:
https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html



Manipulating Spark Streaming
============================

DStream was built upon Spark’s RDD API. DStream is made up of series of microbatch RDDs.


1) Creating A Streaming Context

Streaming context needs a SparkContext object (available as variable sc) and a Duration object (specifies time intervals at which Spark Streaming should split the input stream data) and create mini-batch RDDs.

scala> import org.apache.spark._
scala> import org.apache.spark.streaming._
scala> val ssc = new StreamingContext(sc, Seconds(5))


2) Creating A DStream

DStream is a basic abstraction in Spark Streaming, a sequence of RDDs periodically created from the input stream. 

scala> val filestream = ssc.textFileStream("/home/spark/ch06input")

filestream is an instance of class DStream. DStreams are lazily evaluated like RDDs.


3) Manipulating DStreams

import java.sql.Timestamp 
case class Order(time: java.sql.Timestamp, orderId:Long, 
                 clientId:Long, symbol:String, amount:Int, price:Double, buy:Boolean)

import java.text.SimpleDateFormat
val orders = filestream.flatMap(line => {
   val dateFormat = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss")
   val s = line.split(",")
   try {
      assert(s(6) == "B" || s(6) == "S")
      List(Order(new Timestamp(dateFormat.parse(s(0)).getTime()), s(1).toLong, s(2).toLong, s(3), s(4).toInt, s(5).toDouble, s(6) == "B"))
   }
   catch {
      case e : Throwable => println("Wrong line format ("+e+"): "+line)
      List()
   }
})

val numPerType = orders.map(o => (o.buy, 1L)).reduceByKey((c1, c2) => c1+c2)


4) Saving Results To A File

scala> numPerType.repartition(1).saveAsTextFiles("/home/spark/ch06output/output", "txt")
 

5) Starting Streaming Computation

scala> ssc.start()

scala> ssc.awaitTermination()


6) Computation State Over Time

Spark Streaming can perform calculations taking data from previous mini-batches. Spark Streaming keep track of a state that persists over time and over different mini-batches.

For this, DStreams needs to be converted to PairDStreamFunctions; in other words, DStreams containing key-value tuples.

* updateStateByKey(func: (Seq[V], Option[S]) => Option[S]): DStream[(K, Option[S])]

val amountState = amountPerClient.updateStateByKey((vals, totalOpt) => { 
   totalOpt match {
      case Some(total) => vals.sum + total
      case None => vals.sum 
   }
})

* mapWithState (func: (Time, KeyType, Option[ValueType], State[StateType]) => Option[MappedType])


7) Time-Limited Calculations

Windowed operation is determined by window duration and the slide of the window. Calculation is time-limited and considers only previous mini-batches inside window duration.

val stocksPerWindow = orders.
 map(x => (x.symbol, x.amount)).window(Minutes(60))
 reduceByKey((a1:Int, a2:Int) => a1+a2)


* window(winDur, [slideDur])
* countByWindow(winDur, slideDur)
* countByValueAndWindow(winDur, slideDur, [numParts])
* reduceByWindow(reduceFunc, winDur, slideDur)
* reduceByWindow(reduceFunc, invReduceFunc, winDur, slideDur)
* groupByKeyAndWindow(winDur, [slideDur], [numParts/partitioner])
* reduceByKeyAndWindow(reduceFunc,winDur, [slideDur], [numParts/partitioner])
* reduceByKeyAndWindow(reduceFunc, invReduceFunc, winDur,[slideDur], [numParts],[filterFunc])


8) Specifying Checkpointing Directory

scala> sc.setCheckpointDir("/home/spark/checkpoint")

This is necessary for DStreams resulting from the updateStateByKey method, because updateStateByKey expands RDD’s DAG in each mini-batch, and that can quickly lead to stack overflow exceptions.


9) Other Built-In Input Streams

* FILE INPUT STREAMS

- binaryRecordsStream() reads binary files in records of a specified size (you pass to it a folder name and the records’ size) and returns a DStream containing arrays of bytes (Array[Byte])

- fileStream() requires to parameterize it with classes for key type, value type, and input format (a subclass of Hadoop’s
NewInputFormat) for reading HDFS files. The resulting DStream will contain tuples with two elements of the specified key and value types

* SOCKET INPUT STREAMS

- socketTextStream() returns a DStream whose elements are text lines, delimited by newline characters

- socketStream() needs a function for converting Java InputStreamobjects (for reading binary data) into target objects that will be elements of the resulting DStream.


10) External data sources

Kafka is a distributed, fast, scalable publish-subscribe messaging system. It persists all messages and is capable of acting as a replay queue.

To use Kafka, you need to add Kafka library and Spark Kafka connector library to its classpath.

spark-shell --master local[4] --packages org.apache.spark:spark-
➥ streaming-kafka-0-8_2.11:1.6.1,org.apache.kafka:kafka_2.11:0.8.2.1

* USING THE SPARK KAFKA CONNECTOR

val kafkaReceiverParams = Map[String, String](
 "metadata.broker.list" -> "192.168.10.2:9092")
 
import org.apache.spark.streaming.kafka.KafkaUtils
val kafkaStream = KafkaUtils.
 createDirectStream[String, String, StringDecoder, StringDecoder](ssc, 
 kafkaReceiverParams, Set("orders")
 
* WRITING MESSAGES TO KAFKA

This is accomplished with the DStream foreachRDD method. You can use it to perform an arbitrary action on each RDD in a DStream.

def foreachRDD(foreachFunc: RDD[T] => Unit): Unit
def foreachRDD(foreachFunc: (RDD[T], Time) => Unit): Unit

For instance,

finalStream.foreachRDD((rdd) => {
   val prop = new java.util.Properties
   prop.put("metadata.broker.list", "192.168.10.2:9092") 
   rdd.foreachPartition((iter) => {
      val p = new Producer[Array[Byte], Array[Byte]](new ProducerConfig(prop))
      iter.foreach(x => p.send(new KeyedMessage("metric", x.toString.toCharArray.map(_.toByte))))
      p.close() 
   })
}


Structured Streaming
====================

Structured Streaming uses same Dataframe/Dataset API and Spark SQL to support incremental processing of data stream.

declare a schema to read
--------------------------

val mobileDataSchema = new StructType().add("id", StringType, false)
                       .add("action", StringType, false)
                       .add("ts", TimestampType, false)


create streaming DF from data source
------------------------------------

// file
spark.readStream.format("json").
                 schema(fileSchema).
				 load()
// socket
spark.readStream.format("socket").
                 option("host","localhost").
				 option("port",9999).
				 load()
// kafka
spark.readStream.format("kafka").
                 option("kafka.bootstrap.servers", "host1:port1,host2:port2").
				 option("subscribe", "events").
				 load()
	  
write table result to data sink
-------------------------------

format():
=========
df.writeStream.format("<type>").
               outputMode("<mode>").
			   trigger(Trigger.ProcessingTime("5 second").
			   option("checkpointLocation", checkpointDir).
			   start()

format() :
kafka - inform following options:
        .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
        .option("subscribe", "events")
file (json,csv,parquet,orc) - common destinations local FS, HDFS, or S3
console - print output on console
memory - use driver's memory to store the output

outputMode() :
append - only new rows in result table is written (stateless queries)
complete - entire result table is written (stateful queries)
update - only new/updated rows in result table is written (stateful queries)
			 
		* stateful - queries with groupBy, joins and aggregations
		  stateless - queries that only cleans, transforms or restructures data 

trigger() :
default - process as soon as previous micro-batched is processed
ProcessingTime - process at fixed user-provided interval
once - process whole batch data once

foreachBatch():
==============
def writeCountsToCassandra(updatedCountsDF: DataFrame, batchId: Long) {
 // Use Cassandra batch data source to write the updated counts
 updatedCountsDF
 .write
 .format("org.apache.spark.sql.cassandra")
 .options(Map("table" -> tableName, "keyspace" -> keyspaceName))
 .mode("append")
 .save()

df.writeStream
 .foreachBatch(writeCountsToCassandra _)
 .outputMode("update")
 .option("checkpointLocation", checkpointDir)
 .start()

foreach():
=========
import org.apache.spark.sql.ForeachWriter
val foreachWriter = new ForeachWriter[String] { // typed with Strings
  def open(partitionId: Long, epochId: Long): Boolean = {
  // Open connection to data store
  // Return true if write should continue
  }
   def process(record: String): Unit = {
  // Write string to data store using opened connection
  }
  def close(errorOrNull: Throwable): Unit = {
  // Close the connection
  }
}

resultDSofStrings.writeStream.foreach(foreachWriter).start()


get status streaming DF
-----------------------

query.lastProgress()
query.status()


handling event-time events
--------------------------

event-time window
=================
sensorReadings
 .groupBy("sensorId", window("eventTime", "5 minute"))
 .count()
 
event-time window with sliding 
==============================
sensorReadings
 .groupBy("sensorId", window("eventTime", "10 minute", "5 minute"))
 .count()


event-time window with sliding and watermark
============================================
sensorReadings
 .withWatermark("eventTime", "10 minutes")
 .groupBy("sensorId", window("eventTime", "10 minutes", "5 minute"))
 .mean("value")