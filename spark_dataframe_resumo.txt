Source for more details:
https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html


Spark Dataframes
================

1) Creating SparkSession

import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder().getOrElse()


2) Creating Dataframes

val itPostsRows = sc.textFile("first-edition/ch05/italianPosts.csv")
val itPostsSplit = itPostsRows.map(x => x.split("~"))

* implicit toDF()

import spark.implicits._
import java.sql.Timestamp

case class Post(
	 commentCount:Option[Int], 
	 lastActivityDate:Option[java.sql.Timestamp], 
	 ownerUserId:Option[Long], 
	 body:String, 
	 score:Option[Int], 
	 creationDate:Option[java.sql.Timestamp], 
	 viewCount:Option[Int], 
	 title:String, 
	 tags:String, 
	 answerCount:Option[Int], 
	 acceptedAnswerId:Option[Long], 
	 postTypeId:Option[Long], 
	 id:Long)
 
val caseRDD = itPostsSplit.map(x => Post(x(0).toInt, x(1), x(2).toLong, ..., x(N-1).toLong, x(N).toLong))
val itPosts = caseRDD.toDF()

* via spark.createDataframe

import org.apache.spark.sql.types._
val schema = StructType(Seq( StructField("commentCount", IntegerType, true), 
                             StructField("lastActivityDate", TimestampType, true), 
                             StructField("ownerUserId", LongType, true),
                             ...
                             StructField("postTypeId", LongType, true),
                             StructField("id", LongType, false))
                        )
						
val schema = StructType().add("commentCount", IntegerType).
                          add("lastActivityDate", TimestampType).
                          add("ownerUserId", LongType).
                          ...
                          add("postTypeId", LongType).
                          add("id", LongType)
						  
val rowRDD = itPostsSplit.map(x => Row(x(0),x(1),x(2),....,x(N-1),x(N)))						  
val itPosts = spark.createDataFrame(rowRDD, schema)

--------------------

* df.printSchema - prints schema information
* df.columns - returns a list of column names as Array[String]
* df.dtypes - returns a list of tuples, each containing the column name and the name of its type as Array[(String, String)]

--------------------

Referecing columns in Dataframes

* col("colname") 
* column("colname")
* 'colname - sugar way of constructing a Column class in Scala
* $"colname" - sugar way of constructing a Column class in Scala
* df.col("colname")

--------------------

* df.select(col1, col2, ..., colN)
* df.drop(col1, col2, ..., colN)
* df.filter(expr) or df.where(expr)
* df.withColumn(col, expr)
* df.withColumnRename(col, "new-name")
* df.orderBy(col1, col2, ..., colN) or 
  df.sort(col1, col2, ..., colN)

--------------------

import org.apache.spark.sql.functions._

* Date time - unix_timestamp, from_unixtime, to_date, current_date, current_timesatmp, date_add, date_sub, add_months, datediff, months_
between, dayofmonth, dayofyear, weekofyear, second, minute, hour, month
* String - concat, length, levenshtein, locate, lower, upper, ltrim, rtrim, trim, lpad, rpad, repeat, reverse, split, substring, base64
* Math - cos, acos, sin, asin, tan, atan, ceil, floor, exp, factorial, log, pow, radian, degree, sqrt, hex, unhex
Cryptography - cr32, hash, md5, sha1, sha2
* Aggregation - approx._count_distinct, countDistinct, sumDistinct, avg, corr, count, first, last, max, min, skewness, sum
* Collection - array_contain, explode, from_json, size, sort_array, to_json
* Window - dense_rank, lag, lead, ntile, rank, row_number
* Miscellaneous - coalesce, isNan, isnull, isNotNull, monotonically_increasing_id, lit, when

--------------------

import org.apache.spark.sql.expressions.Window 

Window specification - Window.partitionBy().orderBy().[ rowsBetween(), rangeBetween() ]

- rowsBetween() - restricts rows by their row index, where index 0 is the row being processed, -1 is the previous row, and so on. 
- rangeBetween() - restricts rows by their values and includes only those rows whose values (in the defined column to which the window specification applies) fall in the defined range.

postsDf.filter('postTypeId === 1).
 select('ownerUserId, 'acceptedAnswerId, 'score, max('score).
 over(Window.partitionBy('ownerUserId)) as "maxPerUser").
 withColumn("toMax", 'maxPerUser - 'score).show(10)

postsDf.filter('postTypeId === 1).
 select('ownerUserId, 'id, 'creationDate, 
 lag('id, 1).over(Window.partitionBy('ownerUserId).orderBy('creationDate)) as "prev", 
 lead('id, 1).over(Window.partitionBy('ownerUserId).orderBy('creationDate)) as "next").
 orderBy('ownerUserId, 'id).show(10)

--------------------

Handling missing values

Available at df.na field, returning DataFrameNaFunctions class

* drop() or drop("any") - drop row when null is any column.
* drop("all") - drop row when null is all of columns.
* drop(Array("col1","col2",...,"colN")) - drop row when null is on columns specified.
* fill(value: type) - fills null columns with value matching the type
* replace(Array("col1","col2",...,"colN"), Map(value1 -> value2)) - replaces value1 by value2 in columns listed

--------------------

df.rdd - returns a RDD[org.apache.spark.sql.Row]

Row object has various get* functions for accessing column values by column indexes (getString(index), getInt(index), getMap(index), ...)

--------------------

df.groupBy() - returns RelationGroupedDataset grouping rows that have same values in the grouped columns
             - aggregation functions available are avg(), count(), min(), max(), sum() and agg()

* df.groupBy(col1,col2,...,colN) - creates groups based on values of input columns only
* df.rollup(col1,col2,...,colN) - respects the hierarchy of input columns
* df.cube(col1,col2,...,colN) - returns all combinations of input columns

scala> smplDf.groupBy('ownerUserId, 'tags, 'postTypeId).count.show()

--------------------

* df.join(df2, "col_name") - performs an inner join with "col_name" common to both dataframes
* df.join(df2, col_expression, join_type) - join_type can be inner, outer, left_outer, right_outer, or leftsemi

val postsVotes = postsDf.join(votesDf, postsDf("id") === 'postId)


SparkSQL
========

SparkSQL supports standard ANSI SQL 2003 and HiveQL.

- Enable Hive support in Spark (for HiveQL and Hive metastore and tables)

val spark = SparkSession.builder().
 enableHiveSupport().
 getOrCreate()

- Spark Catalog

Dataframes can be registered as tables in Catalog. 
Catalog stores the table definition (where and how DataframeÂ´s data is stored).
HiveContext creates a Derby database under metastore_db. To change the directory is located, set the hive.metastore.warehouse.dir property in your hive-site.xml file.

-------------

- Loading Dataframe

Built-in data sources: CSV, JSON, Parquet, JDBC, ORC

format: DataFrameReader.format().option().schema().load()

val postsDf = spark.read.format("json").load("postsjson")
val postsDf = spark.read.json("postsjson")

val postsDf = spark.read.table("posts")

val props = new java.util.Properties()
props.setProperty("user", "user")
props.setProperty("password", "password")
val result = spark.read.jdbc("jdbc:postgresql://postgresrv/mydb", "posts", Array("viewCount > 3"), props)

--------------

- Manipulate Tables

postsDf.createOrReplaceTempView("posts_temp")  // creates a temp table in Spark driver
postsDf.write.saveAsTable("posts")  // creates a permanent table. 
                                    // stored in spark_warehouse ( master = local[*] ). can be changed setting the spark.sql.warehouse.dir

val resultDf = spark.sql("select * from posts")  // execute SQL on registered table

--------------

- Saving Dataframes

Built-in data sources: CSV, JSON, Parquet, JDBC, ORC

format: DataFrameWriter.format().mode().option().partitionBy()

* saveAsTable("table")

postsDf.write.saveAsTable("posts")
postsDf.write.format("json").saveAsTable("postsjson")

* insertInto("table")
* save("path")
* jdbc("URL", "table", java.util.Properties)

val props = new java.util.Properties()
props.setProperty("user", "user")
props.setProperty("password", "password")
postsDf.write.jdbc("jdbc:postgresql://postgresrv/mydb", "posts", props)

