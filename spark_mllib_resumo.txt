Source for more details:
https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html


Spark MLlib API
===============

MLlib API (introduced in Spark 0.8) works on RDD API to provide ML functionalities.


- load data

import org.apache.spark.mllib.linalg.Vectors
val housingLines = sc.textFile("first-edition/ch07/housing.data", 6)
val housingVals = housingLines.map(x => Vectors.dense(x.split(",").map(_.trim().toDouble)))

- column statistics

import org.apache.spark.mllib.linalg.distributed.RowMatrix
val housingMat = new RowMatrix(housingVals)
val housingStats = housingMat.computeColumnSummaryStatistics()  # returns a MultivariateStatisticalSummary object

returns housingStats.min / max / mean / variance / normL1 / normL2

- column cosine similarities

val housingColSims = housingMat.columnSimilarities()

- covariance matrix

val housingCovar = housingMat.computeCovariance()


- convert to LabeledPoint

most of Spark’s machine-learning algorithms use LabeledPoint class. 
It contains target value and feature Vector.

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
val housingData = housingVals.map(x => { 
   val a = x.toArray
   LabeledPoint(a(a.length-1), Vectors.dense(a.slice(0, a.length-1))) 
})


- split the dataset

val sets = housingData.randomSplit(Array(0.8, 0.2))
val housingTrain = sets(0)
val housingValid = sets(1)


- Feature scaling / mean normalization

* Feature scaling means the ranges of data are scaled to comparable sizes.
* Mean normalization means the data is translated so that the averages are roughly zero.

import org.apache.spark.mllib.feature.StandardScaler 
val scaler = new StandardScaler(true, true).fit(housingTrain.map(x => x.features)

val trainScaled = housingTrain.map(x => LabeledPoint(x.label, scaler.transform(x.features)))
val validScaled = housingValid.map(x => LabeledPoint(x.label, scaler.transform(x.features))


- fit a linear regression model

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val model = LinearRegressionWithSGD.train(trainScaled, 200, 1.0)

or

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true) 
alg.optimizer.setNumIterations(200)
trainScaled.cache() 
validScaled.cache() 
val model = alg.run(trainScaled)


- predict & evaluate model performance

val validPredicts = validScaled.map(x => (model.predict(x.features), x.label))

import org.apache.spark.mllib.evaluation.RegressionMetrics 
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError
res1: Double = 4.775608317676729

validMetrics.meanSquaredError
res2: Double = 22.806434803863162


- save/load the model

model.save(sc, "chapter07output/model")

import org.apache.spark.mllib.regression.LinearRegressionModel
val model = LinearRegressionModel.load(sc, "ch07output/model")



Spark ML API
============

ML API (introduced in Spark 1.2) uses DataFrame to work with datasets.

ML API introduces new abstractions — estimators, transformers, and evaluators — that can be combined to form pipelines.


- load data

val df_raw = spark.read.format("csv").
             option("header","true").option("inferSchema","true").
             load("first-edition/ch08/adult.raw")

 
- Categorical Values

* StringIndexer() - converts String categorical values into integer indexes by fitting a StringIndexerModel for the column

import org.apache.spark.ml.feature.StringIndexer
val si = new StringIndexer().setInputCol("col").setOutputCol("newcol")
val sm = si.fit(df)
val newdf = sm.transform(df)

* OneHotEncoder() - one-hot encodes a column and puts the results into a new column as sparse Vector.

import org.apache.spark.ml.feature.OneHotEncoder
val onehotenc = new OneHotEncoder().setInputCol("col").setOutputCol("newcol")
val newdf = onehotenc.transform(df)


- VectorAssembler() - merge all Vectors and columns into a single Vector column containing all the features.

import org.apache.spark.ml.feature.VectorAssembler
val va = new VectorAssembler().setInputCols(df.columns.diff(Array("income"))).setOutputCol("features")
val points = va.transform(df)


- split dataset

val splits = points.randomSplit(Array(0.8, 0.2))
val adulttrain = splits(0).cache()
val adultvalid = splits(1).cache()


- fit a logistic regression model

import org.apache.spark.ml.LogisticRegression
val lr = new LogisticRegression.setRegParam(0.01).setMaxIter(500).setFitIntercept(true)

val lrmodel = lr.fit(adulttrain)


- predict & evaluating model performance

val validpredicts = lrmodel.transform(adultvalid)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(validpredicts)
res0: Double = 0.9039934862200736

bceval.getMetricName
res1: String = areaUnderROC


- Precision And Recall Metrics

P =   TP    (precision)
    -------
	TP + FP
	
R =   TP    (recall)
    -------
	TP + FN
	
f1 =   2PR
     -------
	 P + RDD
	 

- PR Curve And ROC Curve

A precision-recall (PR) curve is calculated changing the probability threshold gradually and counting 0 and 1 labels according to probability found at each point, and calculate precision and recall. Then you plot the obtained values on the same graph (precision on the Y-axis and recall on the X-axis)

The ROC curve is similar to the PR curve, but it has recall (TPR) plotted on its Y-axis and the false positive rate (FPR) plotted on its X-axis. FPR is calculated as the percentage of false positives out of all negative samples

FPR =   FP
      -------
	  FP + TN


- save / load the model

* save model / pipeline

lrmodel.write.overwrite().save("/tmp/logistic-regression-model")


* load model / pipeline

val prevModel = LogisticRegressionModel.load("/tmp/spark-logistic-regression-model")


	  
- K-Fold Cross-Validation

import org.apache.spark.ml.tuning.CrossValidator
val cv = new CrossValidator().setEstimator(lr).
    setEvaluator(bceval).setNumFolds(5

import org.apache.spark.ml.tuning.ParamGridBuilder
val paramGrid = new ParamGridBuilder().
    addGrid(lr.maxIter, Array(1000)).
    addGrid(lr.regParam, Array(0.0001, 0.001, 0.005, 0.01, 0.05, 0.1)).
    build()

cv.setEstimatorParamMaps(paramGrid)

val cvmodel = cv.fit(adulttrain

* Getting Best Model And Evaluating

import org.apache.spark.ml.classification.LogisticRegressionModel
val bestmodel = cvmodel.bestModel.asInstanceOf[LogisticRegressionModel]

bestmodel.coefficients
best.get* (RegParam, Threshold, Iterations, etc)

val eval = new BinaryClassificationEvaluator()
eval.evaluate(bestmodel.transform(adultvalid))


- other Transformers and Estimators

* TRANSFORMERS

- Binarizer() - transforms input column values into two groups (0/1).

import org.apache.spark.ml.feature.Binarizer
val binarizer = new Binarizer().setThreshold(35.6)
                .setInputCol("temperature").setOutputCol("freezing")
binarizer.transform(arrival_data).show

- Bucketizer() - transforms input column values into buckets specified by an array of double values.

import org.apache.spark.ml.feature.Bucketizer
val bucketer = new Bucketizer().setSplits(bucketBorders)
               .setInputCol("temperature").setOutputCol("intensity")
val output = bucketer.transform(arrival_data)

- Tokenizer() - performs tokenization on a string separated by spaces. If a different delimiter is used, use RegexTokenizer.

import org.apache.spark.ml.feature.Tokenizer
val tokenizer = new Tokenizer().setInputCol("line").setOutputCol("words")
val tokenized = tokenizer.transform(text_data)

- StopWordsRemover() - removes stop words from a built-in or user-provided set after the tokenization.

import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords)
              .setInputCol("words").setOutputCol("filtered")
val cleanedTokens = remover.transform(tokenized)

- HashingTF() - transforms words into a numeric representation (by applying a hash function) and computing the frequency of each word in each line.

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setNumFeatures(4096)
             .setInputCol("filtered").setOutputCol("TFOut")
val tfResult = tf.transform(cleanedTokens)

* ESTIMATORS

- RFormula() - converts both numeric and categorial values to label and feature vector by expressing the transformation logic declaretively. 

import org.apache.spark.ml.feature.RFormula
val formula = new RFormula().setFormula("arrival ~ . + hour:temperature")
              .setFeaturesCol("features").setLabelCol("label")
val output = formula.fit(arrival_data).transform(arrival_data)

- IDF() - computes the importance or weight of each word by counting the number of documents it appears in.

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("wordFreqVect")
          .setOutputCol("features")
val idfModel = idf.fit(tfResult)
val weightedWords = idfModel.transform(tfResult)

- MinMaxScaler() - rescales each column individually to a common range of values of min and max using the column summary statistics.

import org.apache.spark.ml.feature.MinMaxScaler
val minMaxScaler = new MinMaxScaler().setMin(0.0).setMax(5.0)
                   .setInputCol("features").setOutputCol("scaledFeatures")
val scalerModel = minMaxScaler.fit(employee_data)
val scaledData = scalerModel.transform(employee_data)