Source for more details:
https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html



Manipulating Spark Streaming
============================

Spark Streaming was introduced in Spark 1.3 using DStream API (RDD).
 

1) Creating A Streaming Context

Streaming context needs a SparkContext object (available as variable sc) and a Duration object (specifies time intervals at which Spark Streaming should split the input stream data) and create mini-batch RDDs.

scala> import org.apache.spark._
scala> import org.apache.spark.streaming._
scala> val ssc = new StreamingContext(sc, Seconds(5))


2) Creating A DStream

DStream is a basic abstraction in Spark Streaming, a sequence of RDDs periodically created from the input stream. 

scala> val filestream = ssc.textFileStream("/home/spark/ch06input")

filestream is an instance of class DStream. DStreams are lazily evaluated like RDDs.


3) Manipulating DStreams

import java.sql.Timestamp 
case class Order(time: java.sql.Timestamp, orderId:Long, 
                 clientId:Long, symbol:String, amount:Int, price:Double, buy:Boolean)

import java.text.SimpleDateFormat
val orders = filestream.flatMap(line => {
   val dateFormat = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss")
   val s = line.split(",")
   try {
      assert(s(6) == "B" || s(6) == "S")
      List(Order(new Timestamp(dateFormat.parse(s(0)).getTime()), s(1).toLong, s(2).toLong, s(3), s(4).toInt, s(5).toDouble, s(6) == "B"))
   }
   catch {
      case e : Throwable => println("Wrong line format ("+e+"): "+line)
      List()
   }
})

val numPerType = orders.map(o => (o.buy, 1L)).reduceByKey((c1, c2) => c1+c2)


4) Saving Results To A File

scala> numPerType.repartition(1).saveAsTextFiles("/home/spark/ch06output/output", "txt")
 

5) Starting Streaming Computation

scala> ssc.start()

scala> ssc.awaitTermination()


6) Computation State Over Time

Spark Streaming can perform calculations taking data from previous mini-batches. Spark Streaming keep track of a state that persists over time and over different mini-batches.

For this, DStreams needs to be converted to PairDStreamFunctions; in other words, DStreams containing key-value tuples.

* updateStateByKey(func: (Seq[V], Option[S]) => Option[S]): DStream[(K, Option[S])]

val amountState = amountPerClient.updateStateByKey((vals, totalOpt) => { 
   totalOpt match {
      case Some(total) => vals.sum + total
      case None => vals.sum 
   }
})

* mapWithState (func: (Time, KeyType, Option[ValueType], State[StateType]) => Option[MappedType])


7) Time-Limited Calculations

Windowed operation is determined by window duration and the slide of the window. Calculation is time-limited and considers only previous mini-batches inside window duration.

val stocksPerWindow = orders.
 map(x => (x.symbol, x.amount)).window(Minutes(60))
 reduceByKey((a1:Int, a2:Int) => a1+a2)


* window(winDur, [slideDur])
* countByWindow(winDur, slideDur)
* countByValueAndWindow(winDur, slideDur, [numParts])
* reduceByWindow(reduceFunc, winDur, slideDur)
* reduceByWindow(reduceFunc, invReduceFunc, winDur, slideDur)
* groupByKeyAndWindow(winDur, [slideDur], [numParts/partitioner])
* reduceByKeyAndWindow(reduceFunc,winDur, [slideDur], [numParts/partitioner])
* reduceByKeyAndWindow(reduceFunc, invReduceFunc, winDur,[slideDur], [numParts],[filterFunc])


7) Specifying Checkpointing Directory

scala> sc.setCheckpointDir("/home/spark/checkpoint")

This is necessary for DStreams resulting from the updateStateByKey method, because updateStateByKey expands RDD’s DAG in each mini-batch, and that can quickly lead to stack overflow exceptions.


8) Other Built-In Input Streams

* FILE INPUT STREAMS

- binaryRecordsStream() reads binary files in records of a specified size (you pass to it a folder name and the records’ size) and returns a DStream containing arrays of bytes (Array[Byte])

- fileStream() requires to parameterize it with classes for key type, value type, and input format (a subclass of Hadoop’s
NewInputFormat) for reading HDFS files. The resulting DStream will contain tuples with two elements of the specified key and value types

* SOCKET INPUT STREAMS

- socketTextStream() returns a DStream whose elements are text lines, delimited by newline characters

- socketStream() needs a function for converting Java InputStreamobjects (for reading binary data) into target objects that will be elements of the resulting DStream.


9) External data sources

Kafka is a distributed, fast, scalable publish-subscribe messaging system. It persists all messages and is capable of acting as a replay queue.

To use Kafka, you need to add Kafka library and Spark Kafka connector library to its classpath.

spark-shell --master local[4] --packages org.apache.spark:spark-
➥ streaming-kafka-0-8_2.11:1.6.1,org.apache.kafka:kafka_2.11:0.8.2.1

* USING THE SPARK KAFKA CONNECTOR

val kafkaReceiverParams = Map[String, String](
 "metadata.broker.list" -> "192.168.10.2:9092")
 
import org.apache.spark.streaming.kafka.KafkaUtils
val kafkaStream = KafkaUtils.
 createDirectStream[String, String, StringDecoder, StringDecoder](ssc, 
 kafkaReceiverParams, Set("orders")
 
* WRITING MESSAGES TO KAFKA

This is accomplished with the DStream foreachRDD method. You can use it to perform an arbitrary action on each RDD in a DStream.

def foreachRDD(foreachFunc: RDD[T] => Unit): Unit
def foreachRDD(foreachFunc: (RDD[T], Time) => Unit): Unit

For instance,

finalStream.foreachRDD((rdd) => {
   val prop = new java.util.Properties
   prop.put("metadata.broker.list", "192.168.10.2:9092") 
   rdd.foreachPartition((iter) => {
      val p = new Producer[Array[Byte], Array[Byte]](new ProducerConfig(prop))
      iter.foreach(x => p.send(new KeyedMessage("metric", x.toString.toCharArray.map(_.toByte))))
      p.close() 
   })
}


Manipulating Structured Streaming
=================================

Structured Streaming was introduced in Spark 1.6 using Structured API (Dataframe and Datasets).


2) Creating A Structured Streaming

import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
val mobileDataSchema = new StructType().add("id", StringType, false)
                       .add("action", StringType, false)
                       .add("ts", TimestampType, false)

val mobileSSDF = spark.readStream.schema(mobileDataSchema).json("/<path>/chapter6/data/input")


3) Manipulating Structured Streaming

val actionCountDF = mobileSSDF.groupBy(window($"ts", "10 minutes"), $"action").count


4) Saving Results To A Sink

val mobileConsoleSQ = actionCountDF.writeStream
 .format("console").option("truncate", "false")
 .outputMode("complete")
 

5) Starting Streaming Computation

mobileConsoleSQ.start()

mobileConsoleSQ.awaitTermination()

mobileConsoleSQ.status
 
mobileConsoleSQ.lastProgres



6) Data Sources

• Kafka source: require Apache Kafka with version 0.10 or higher. 
This is the most popular data source in a production environment. 

• File source: Files are located on either the local file system, HDFS, 
or S3. As new files are dropped into a directory, this data source 
will pick them up for processing. Commonly used file formats are 
supported, such as text, CSV, JSON, ORC, and Parquet. 

• Socket source: This is for testing purposes only. It reads UTF-8 data 
from a socket listening on a certain host and port.

• Rate source: This is for testing and benchmark purposes only. This 
source can be configured to generate a number of events per second, 
where each event consists of a timestamp and a monotonically 
increased value.


7) Output Modes

• Append mode: This is the default mode if output mode is not 
specified. In this mode, only new rows that were appended to the 
result table will be sent to the specified output sink

• Complete mode: The entire result table will be written to the output sink.

• Update mode: Only the rows that were updated in the result table will 
be written to the output sink. For the rows that were not changed, 
they will not be written out


8) Data Sinks

• Kafka sink: require Apache Kafka with version 0.10 or higher. There 
is a specific set of settings to connect to a Kafka cluster. Please refer to 
the Kafka Integration Guide on the Spark website for more details.

• File sink: This is a destination on a file system, HDFS, or S3. 
Commonly used file formats are supported, such as text, CSV, JSON, 
ORC, and Parquet. See the DataStreamReader interface for an up-todate list of supported file formats.

• Foreach sink: This is meant for running arbitrary computations on the 
rows in the output.

• Console sink: This is for testing and debugging purposes only and 
when working with low-volume data. The output is printed to the 
console on every trigger.

• Memory sink: This is for testing and debugging purposes only when 
working with low-volume data. It uses the memory of the driver to 
store the output


9) Trigger Types

* Not specified(default): For this default type, Spark will use the micro-batch mode and process the next 
batch of data as soon as the previous batch of data has completed processing.

* Fixed interval: For this type, Spark will use the micro-batch mode and process the batch of data 
based on the user-provided interval. If for whatever reason the processing of the 
previous batch of data takes longer than the interval, then the next batch of data 
is processed immediately after the previous one is completed.

* One-time:  This trigger type is meant to be used for one-time processing of the available 
batch of data, and Spark will immediately stop the streaming application once 
the processing is completed.