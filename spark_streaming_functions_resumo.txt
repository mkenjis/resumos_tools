Source for more details:
https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html


Manipulating Spark Streaming
============================

1) Creating A Streaming Context

Streaming context needs a SparkContext object (available as variable sc) and a Duration object (specifies time intervals at which Spark Streaming should split the input stream data) and create mini-batch RDDs.

scala> import org.apache.spark._
scala> import org.apache.spark.streaming._
scala> val ssc = new StreamingContext(sc, Seconds(5))


2) Creating A DStream

DStream is a basic abstraction in Spark Streaming, a sequence of RDDs periodically created from the input stream. 

scala> val filestream = ssc.textFileStream("/home/spark/ch06input")

filestream is an instance of class DStream. DStreams are lazily evaluated like RDDs.


3) Manipulating DStreams

import java.sql.Timestamp 
case class Order(time: java.sql.Timestamp, orderId:Long, 
                 clientId:Long, symbol:String, amount:Int, price:Double, buy:Boolean)

import java.text.SimpleDateFormat
val orders = filestream.flatMap(line => {
   val dateFormat = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss")
   val s = line.split(",")
   try {
      assert(s(6) == "B" || s(6) == "S")
      List(Order(new Timestamp(dateFormat.parse(s(0)).getTime()), s(1).toLong, s(2).toLong, s(3), s(4).toInt, s(5).toDouble, s(6) == "B"))
   }
   catch {
      case e : Throwable => println("Wrong line format ("+e+"): "+line)
      List()
   }
})

val numPerType = orders.map(o => (o.buy, 1L)).reduceByKey((c1, c2) => c1+c2)


4) Saving Results To A File

scala> numPerType.repartition(1).saveAsTextFiles("/home/spark/ch06output/output", "txt")
 

5) Starting Streaming Computation

scala> ssc.start()

scala> ssc.awaitTermination()


6) Computation State Over Time

Spark Streaming can perform calculations taking data from previous mini-batches. Spark Streaming keep track of a state that persists over time and over different mini-batches.

For this, DStreams needs to be converted to PairDStreamFunctions; in other words, DStreams containing key-value tuples.

* updateStateByKey(func: (Seq[V], Option[S]) => Option[S]): DStream[(K, Option[S])]

val amountState = amountPerClient.updateStateByKey((vals, totalOpt) => { 
   totalOpt match {
      case Some(total) => vals.sum + total
      case None => vals.sum 
   }
})

* mapWithState (func: (Time, KeyType, Option[ValueType], State[StateType]) => Option[MappedType])


7) Time-Limited Calculations

Windowed operation is determined by window duration and the slide of the window. Calculation is time-limited and considers only previous mini-batches inside window duration.

val stocksPerWindow = orders.
 map(x => (x.symbol, x.amount)).window(Minutes(60))
 reduceByKey((a1:Int, a2:Int) => a1+a2)


* window(winDur, [slideDur])
* countByWindow(winDur, slideDur)
* countByValueAndWindow(winDur, slideDur, [numParts])
* reduceByWindow(reduceFunc, winDur, slideDur)
* reduceByWindow(reduceFunc, invReduceFunc, winDur, slideDur)
* groupByKeyAndWindow(winDur, [slideDur], [numParts/partitioner])
* reduceByKeyAndWindow(reduceFunc,winDur, [slideDur], [numParts/partitioner])
* reduceByKeyAndWindow(reduceFunc, invReduceFunc, winDur,[slideDur], [numParts],[filterFunc])


7) Specifying Checkpointing Directory

scala> sc.setCheckpointDir("/home/spark/checkpoint")

This is necessary for DStreams resulting from the updateStateByKey method, because updateStateByKey expands RDD’s DAG in each mini-batch, and that can quickly lead to stack overflow exceptions.


8) Other Built-In Input Streams

* FILE INPUT STREAMS

- binaryRecordsStream() reads binary files in records of a specified size (you pass to it a folder name and the records’ size) and returns a DStream containing arrays of bytes (Array[Byte])

- fileStream() requires to parameterize it with classes for key type, value type, and input format (a subclass of Hadoop’s
NewInputFormat) for reading HDFS files. The resulting DStream will contain tuples with two elements of the specified key and value types

* SOCKET INPUT STREAMS

- socketTextStream() returns a DStream whose elements are text lines, delimited by newline characters

- socketStream() needs a function for converting Java InputStreamobjects (for reading binary data) into target objects that will be elements of the resulting DStream.


9) External data sources

Kafka is a distributed, fast, scalable publish-subscribe messaging system. It persists all messages and is capable of acting as a replay queue.

To use Kafka, you need to add Kafka library and Spark Kafka connector library to its classpath.

spark-shell --master local[4] --packages org.apache.spark:spark-
➥ streaming-kafka-0-8_2.11:1.6.1,org.apache.kafka:kafka_2.11:0.8.2.1

* USING THE SPARK KAFKA CONNECTOR

val kafkaReceiverParams = Map[String, String](
 "metadata.broker.list" -> "192.168.10.2:9092")
 
import org.apache.spark.streaming.kafka.KafkaUtils
val kafkaStream = KafkaUtils.
 createDirectStream[String, String, StringDecoder, StringDecoder](ssc, 
 kafkaReceiverParams, Set("orders")
 
* WRITING MESSAGES TO KAFKA

This is accomplished with the DStream foreachRDD method. You can use it to perform an arbitrary action on each RDD in a DStream.

def foreachRDD(foreachFunc: RDD[T] => Unit): Unit
def foreachRDD(foreachFunc: (RDD[T], Time) => Unit): Unit

For instance,

finalStream.foreachRDD((rdd) => {
   val prop = new java.util.Properties
   prop.put("metadata.broker.list", "192.168.10.2:9092") 
   rdd.foreachPartition((iter) => {
      val p = new Producer[Array[Byte], Array[Byte]](new ProducerConfig(prop))
      iter.foreach(x => p.send(new KeyedMessage("metric", x.toString.toCharArray.map(_.toByte))))
      p.close() 
   })
}